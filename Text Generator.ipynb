{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generator\n",
    "\n",
    "We are going to develop a text generator based on Markov Chains, and try different inputs and hiperparameters in order to achieve concrete outcomes that help us understand the possibilities of this technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to model the problem with some classes in order to make the development easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.count_following_tokens = 0\n",
    "        self.following_tokens = {}\n",
    "\n",
    "    def add_following_token(self, other_token):\n",
    "        self.following_tokens[other_token] = self.following_tokens.get(other_token, 0) + 1\n",
    "        self.count_following_tokens += 1\n",
    "        \n",
    "    def generate_next_token(self):\n",
    "        next_tokens = list(self.following_tokens.keys())\n",
    "        next_tokens_prob = list(map(lambda x: x / self.count_following_tokens, list(self.following_tokens.values())))\n",
    "        if len(next_tokens) == 0:\n",
    "            return None\n",
    "        return choice(next_tokens, 1, p = next_tokens_prob)[0]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \" \".join(list(map(str, self.words)))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \" \".join(list(map(str, self.words)))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(\"\".join(list(map(str, self.words))))\n",
    "\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return type(other) == Token and self.words == other.words\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return type(other) != Token or self.words != other.words\n",
    "        \n",
    "        \n",
    "class Word:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.word)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return type(other) == Word and self.word == other.word\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return type(other) != Word or self.word != other.word\n",
    "        \n",
    "class TextGenerator:\n",
    "    def __init__(self, text, token_size):\n",
    "        self.words_dict = {}\n",
    "        self.words_list = []\n",
    "        self.tokens_dict = {}\n",
    "        self.tokens_list = []\n",
    "        self.common_words = [\"de\", \"la\", \"el\", \"en\", \"y\", \"del\", \"los\", \"a\", \"las\", \"que\", \"por\", \"se\", \"con\", \"un\", \"como\", \"una\", \"más\", \"es\", \"al\", \"su\", \"entre\", \"fue\", \"o\", \"también\", \"para\", \"son\", \"ha\", \"este\", \"desde\", \"lo\", \"sus\", \"si\", \"no\", \"uno\", \"sobre\", \"dos\", \"hasta\", \"durante\", \"the\", \"to\", \"and\", \"i\", \"you\", \"of\", \"we\", \"it\", \"that\", \"have\", \"they\", \"in\", \"going\", \"so\", \"is\", \"but\", \"were\", \"its\", \"be\", \"are\", \"for\", \"our\", \"this\", \"dont\", \"was\", \"do\", \"not\", \"with\", \"he\", \"im\", \"all\", \"what\", \"because\", \"want\", \"me\", \"said\", \"theyre\", \"on\", \"very\", \"about\", \"like\", \"if\", \"one\", \"them\", \"will\", \"at\", \"thats\", \"just\", \"now\", \"by\", \"when\", \"up\", \"these\", \"look\", \"been\", \"go\", \"from\", \"lot\", \"can\", \"know\", \"got\", \"many\", \"there\", \"my\", \"had\", \"good\", \"well\", \"make\", \"an\", \"or\", \"get\", \"think\", \"right\", \"out\", \"us\", \"really\", \"has\", \"hes\", \"as\", \"way\", \"mean\", \"much\", \"would\", \"over\", \"even\", \"who\", \"take\", \"youre\", \"ive\", \"than\", \"some\", \"never\", \"tell\", \"she\", \"doing\", \"time\", \"then\", \"say\", \"see\", \"other\", \"how\", \"their\", \"cant\", \"more\", \"didnt\", \"did\", \"things\", \"thing\", \"him\", \"where\", \"ever\", \"your\", \"into\", \"okay\", \"which\", \"something\", \"here\", \"need\", \"guy\", \"could\", \"ill\", \"why\", \"talk\", \"down\", \"done\", \"bad\", \"those\", \"actually\", \"weve\", \"let\", \"better\", \"ago\", \"new\", \"only\", \"came\", \"oh\", \"s\", \"apap\", \"ap\", \"unk\"]\n",
    "        self.available_characters = \"abcdefghijklmnñopqrstuvwxyzáéíóú\"\n",
    "        self.text = text\n",
    "        self.token_size = token_size\n",
    "        \n",
    "    def process_text(self):\n",
    "        self.split_words()\n",
    "        self.generate_tokens()\n",
    "        self.calculate_probabilities()\n",
    "        \n",
    "    def split_words(self):\n",
    "        words = self.text.split()\n",
    "        for word in words:\n",
    "            word = self.curate_word(word)\n",
    "            if (word == \"\"):\n",
    "                continue\n",
    "            self.words_dict[word] = self.words_dict.get(word, Word(word))\n",
    "            self.words_list.append(self.words_dict[word])\n",
    "\n",
    "    def generate_tokens(self):\n",
    "        for i in range(0, (len(self.words_list) // self.token_size) * self.token_size, self.token_size):\n",
    "            previous_words = tuple([self.words_list[j] for j in range(i, i + self.token_size)])\n",
    "            self.tokens_dict[previous_words] = self.tokens_dict.get(previous_words, Token(previous_words))\n",
    "            self.tokens_list.append(self.tokens_dict[previous_words])\n",
    "\n",
    "    def calculate_probabilities(self):\n",
    "        for i in range(len(self.tokens_list) - 1):\n",
    "            self.tokens_list[i].add_following_token(self.tokens_list[i + 1])\n",
    "\n",
    "    def curate_word(self, word):\n",
    "        word = word.lower()\n",
    "        curated_word = \"\"\n",
    "        for c in word:\n",
    "            if c in self.available_characters:\n",
    "                curated_word += c\n",
    "        return curated_word\n",
    "    \n",
    "    def generate_text(self, length):\n",
    "        generated_text = [self.pick_random_token()]\n",
    "        for i in range(length - 1):\n",
    "            next_token = generated_text[i].generate_next_token()\n",
    "            if next_token == None:\n",
    "                next_token = self.pick_random_token()\n",
    "            generated_text.append(next_token)\n",
    "        return ' '.join(list(map(str, generated_text)))\n",
    "        \n",
    "    def pick_random_token(self):\n",
    "        return choice(list(self.tokens_dict.values()))\n",
    "    \n",
    "    def plot_most_important_words(self, n):\n",
    "        counter = Counter(self.words_list)    \n",
    "        words = sorted(counter.items(), key = lambda x: -x[1])\n",
    "        most_important_words = []\n",
    "        most_important_words_count = []\n",
    "        for word in words:\n",
    "            if word[0].word not in self.common_words:\n",
    "                most_important_words.append(word[0].word)\n",
    "                most_important_words_count.append(word[1])\n",
    "                if (len(most_important_words) == n):\n",
    "                    break\n",
    "        fig = plt.figure(figsize=(30,10))\n",
    "        plt.bar(most_important_words, most_important_words_count, orientation='vertical')\n",
    "        fig.suptitle('Most important words', fontsize=40)\n",
    "        plt.xticks(rotation=45, fontsize=20)\n",
    "        \n",
    "    def number_of_words(self):\n",
    "        return len(self.words_list)\n",
    "    \n",
    "    def number_of_different_words(self):\n",
    "        return len(self.words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textGenerator = TextGenerator('a b a c a c a c d', 2)\n",
    "textGenerator.process_text()\n",
    "textGenerator.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Página de Argentina en Wikipedia\n",
    "A continuación alimentaremos el generador de texto con la página de Argentina de Wikipedia en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argentina_file = open(\"Argentina.txt\")\n",
    "argentina_content = \" \".join(argentina_file.readlines())\n",
    "textGeneratorArgentina = TextGenerator(argentina_content, 1)\n",
    "textGeneratorArgentina.process_text()\n",
    "\n",
    "number_of_words = textGeneratorArgentina.number_of_words()\n",
    "number_of_different_words = textGeneratorArgentina.number_of_different_words()\n",
    "print(\"Number of words: \" + str(number_of_words) + \". Number of different words: \" + str(number_of_different_words))\n",
    "\n",
    "textGeneratorArgentina.plot_most_important_words(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta la palabra anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorArgentina = TextGenerator(argentina_content, 2)\n",
    "textGeneratorArgentina.process_text()\n",
    "textGeneratorArgentina.generate_text(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* alvear el objeto de américa del país latinoamericano en la provincia de infraestructura entre otros\n",
    "* espaciadas entre del precepto alberdiano de un país se vota por vicente lópez con los\n",
    "* épico se encuentra en la renacionalización de gobernar es el instituto balseiro ubicado en bariloche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las dos palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorArgentina = TextGenerator(argentina_content, 2)\n",
    "textGeneratorArgentina.process_text()\n",
    "textGeneratorArgentina.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* las letras del tango pero ha perdido buena parte de la alianza fue encabezada por el indec las provincias sus\n",
    "* tanto públicas como privadas que se desarrollan también construye helicópteros maquinarias agrícolas produce el ciclo completo de la soja la\n",
    "* ascendencia europea es menor y que constituye uno de los hombres obtuvieron el título más importante de la historia argentina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las tres palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorArgentina = TextGenerator(argentina_content, 3)\n",
    "textGeneratorArgentina.process_text()\n",
    "textGeneratorArgentina.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* llamados comechingones resistieron con éxito la invasión incaica y se mantuvieron como señoríos independientes a\n",
    "* español andino se fusiona con el dialecto de rioplatense la provincia de buenos aires con\n",
    "* leche es muy importante consumiéndose alrededor de litros por persona por año de la existencia de grandes disponibilidades de leche se ha derivado un alto consumo de alimentos derivados como"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Página de Estados Unidos en Wikipedia\n",
    "A continuación alimentaremos el generador de texto con la página de Estados Unidos en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estados_unidos_file = open(\"EstadosUnidos.txt\")\n",
    "estados_unidos_content = \" \".join(estados_unidos_file.readlines())\n",
    "textGeneratorEstadosUnidos = TextGenerator(estados_unidos_content, 2)\n",
    "textGeneratorEstadosUnidos.process_text()\n",
    "\n",
    "number_of_words = textGeneratorEstadosUnidos.number_of_words()\n",
    "number_of_different_words = textGeneratorEstadosUnidos.number_of_different_words()\n",
    "print(\"Number of words: \" + str(number_of_words) + \". Number of different words: \" + str(number_of_different_words))\n",
    "\n",
    "textGeneratorEstadosUnidos.plot_most_important_words(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta la palabra anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorEstadosUnidos = TextGenerator(estados_unidos_content, 1)\n",
    "textGeneratorEstadosUnidos.process_text()\n",
    "textGeneratorEstadosUnidos.generate_text(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* reubicación de materiales y públicas de igual forma la música artículos principales carreras de julio\n",
    "* cometieron el de influencia de los logros socioeconómicos de los aliados contra el estado islámico\n",
    "* locales ocupaban el estado libre asociado con el feto ya que destacan autores como estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las dos palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorEstadosUnidos = TextGenerator(estados_unidos_content, 2)\n",
    "textGeneratorEstadosUnidos.process_text()\n",
    "textGeneratorEstadosUnidos.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* comercial estadounidense era de millones de afroamericanos que habían sido esclavos convirtiéndolos en ciudadanos y dándoles el derecho consuetudinario en\n",
    "* mundo aunque en términos de gasto per cápita de toneladas de petróleo al año en salarios aunque su legalidad está\n",
    "* transporte de mercancías por ferrocarril es muy importante relativamente pocas personas utilizan el transporte público para acudir al trabajo un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las tres palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorEstadosUnidos = TextGenerator(estados_unidos_content, 3)\n",
    "textGeneratorEstadosUnidos.process_text()\n",
    "textGeneratorEstadosUnidos.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* y al presidente de los estados unidos donde se reúne el congreso estados unidos es una nación multicultural hogar de una amplia variedad de grupos étnicos tradiciones y valores aparte\n",
    "* suelen emplearse de manera correcta la abreviatura ee uu estados unidos o la sigla eua estados unidos de américa aunque frecuente en español es incorrecto emplear la sigla inglesa usa\n",
    "* es considerado un país megadiverso unas especies de plantas vasculares viven en los estados unidos cerca del de los universitarios asisten a colleges públicos como la universidad de virginia un"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Página de China en Wikipedia\n",
    "A continuación alimentaremos el generador de texto con la página de China en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_file = open(\"China.txt\")\n",
    "china_content = \" \".join(china_file.readlines())\n",
    "textGeneratorChina = TextGenerator(china_content, 3)\n",
    "textGeneratorChina.process_text()\n",
    "\n",
    "number_of_words = textGeneratorChina.number_of_words()\n",
    "number_of_different_words = textGeneratorChina.number_of_different_words()\n",
    "print(\"Number of words: \" + str(number_of_words) + \". Number of different words: \" + str(number_of_different_words))\n",
    "\n",
    "textGeneratorChina.plot_most_important_words(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta la palabra anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorChina = TextGenerator(china_content, 1)\n",
    "textGeneratorChina.process_text()\n",
    "textGeneratorChina.generate_text(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* fin del arroz el segundo miembro de taiwán el kuomintang liderado por ejemplo de pekín\n",
    "* incrementó los yuan y animales y los años atrás una vez más poblado del planeta\n",
    "* septiembre de las dos hijos si lo que mantuvieron un el jengibre el muy criticado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las dos palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorChina = TextGenerator(china_content, 2)\n",
    "textGeneratorChina.process_text()\n",
    "textGeneratorChina.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* área territorial después de la creación de la onu china tiene una tasa de mortalidad infantil es de por cada\n",
    "* transitados en el mundo con y millones de chinos en las zonas rurales no tienen acceso a agua potable y\n",
    "* ejército chino transporte el puente de donghai es uno de los mejores baloncestistas de china carreras de barcodragón un deporte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las tres palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorChina = TextGenerator(china_content, 3)\n",
    "textGeneratorChina.process_text()\n",
    "textGeneratorChina.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* mundial y la retirada de sus tropas de china el partido comunista de china pcc cuyo poder está consagrado en la constitución la constitución de la república popular china están\n",
    "* en términos de pib medido en paridad de poder adquisitivo y manteniéndose como la segunda potencia por pib nominal china es además el mayor exportador e importador de bienes y\n",
    "* un país que tiene armas nucleares reconocidas china es considerada una potencia militar regional y una superpotencia militar emergente de acuerdo al informe de del departamento de defensa de estados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discursos de Trump en campaña presidencial\n",
    "\n",
    "Discursos originales de Donald Trump en la campaña presidencial, en inglés. Se agregaron muchas palabras a la la lista de palabras comunes que no se consideraron importantes o representativas para el análisis del discurso\n",
    "\n",
    "Extraídos de https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_file = open(\"Trump.txt\")\n",
    "trump_content = \" \".join(trump_file.readlines())\n",
    "textGeneratorTrump = TextGenerator(trump_content, 1)\n",
    "textGeneratorTrump.process_text()\n",
    "\n",
    "number_of_words = textGeneratorTrump.number_of_words()\n",
    "number_of_different_words = textGeneratorTrump.number_of_different_words()\n",
    "print(\"Number of words: \" + str(number_of_words) + \". Number of different words: \" + str(number_of_different_words))\n",
    "\n",
    "textGeneratorTrump.plot_most_important_words(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta la palabra anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorTrump = TextGenerator(trump_content, 1)\n",
    "textGeneratorTrump.process_text()\n",
    "textGeneratorTrump.generate_text(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* global warming which are they are in by the radar nobody recognizes them iraq was\n",
    "* upper income earned abroad in a picture because were going to bring back in the\n",
    "* henry right now he spends million on something because of words radical islam is standing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las dos palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorTrump = TextGenerator(trump_content, 2)\n",
    "textGeneratorTrump.process_text()\n",
    "textGeneratorTrump.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* any deals we all saw and we witnessed something that you could say well were going after hillary clinton she\n",
    "* means less than zero and i said it very strongly in the meantime in michigan and other places and they\n",
    "* though he was under pressure because theres nothing we have to get rid of the bullets went the opposite direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las tres palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorTrump = TextGenerator(trump_content, 3)\n",
    "textGeneratorTrump.process_text()\n",
    "textGeneratorTrump.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* four or five weeks ill tell you but they got access so they signed a pledge they will support i dont want their support it wont mean one vote than\n",
    "* world take advantage of us both militarily and we dont win anymore we dont win anymore we dont win with trade we dont win we dont win at anything i\n",
    "* extent is the power of weaponry its the power its the tremendous power you know years ago i mean it took them like years to build it so you know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discursos de Hillary Clinton en campaña presidencial\n",
    "\n",
    "Discursos originales de Hillary Clinton en la campaña presidencial, en inglés. Se agregaron muchas palabras a la lista de palabras comunes que no se consideraron importantes o representativas para el análisis del discurso\n",
    "\n",
    "Extraídos de https://votesmart.org/candidate/public-statements/55463/hillary-clinton?speechType=1#.XR_krJNKjOQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hillary_file = open(\"HillaryClinton.txt\")\n",
    "hillary_content = \" \".join(hillary_file.readlines())\n",
    "textGeneratorHillary = TextGenerator(hillary_content, 3)\n",
    "textGeneratorHillary.process_text()\n",
    "\n",
    "number_of_words = textGeneratorHillary.number_of_words()\n",
    "number_of_different_words = textGeneratorHillary.number_of_different_words()\n",
    "print(\"Number of words: \" + str(number_of_words) + \". Number of different words: \" + str(number_of_different_words))\n",
    "\n",
    "textGeneratorHillary.plot_most_important_words(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta la palabra anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorHillary = TextGenerator(hillary_content, 1)\n",
    "textGeneratorHillary.process_text()\n",
    "textGeneratorHillary.generate_text(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* youre wrong with we talk about illegal immigration on that brexits going on a statement\n",
    "* reaping with honor of public schools and we might have more good middle class i\n",
    "* shortchange her own health care of job is the hatefulness attacks one has the armed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las dos palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorHillary = TextGenerator(hillary_content, 2)\n",
    "textGeneratorHillary.process_text()\n",
    "textGeneratorHillary.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* are within reach where families are supported streets are safe and yes we have created with all of this on\n",
    "* work drive hundreds of campuses across the country attacks that disproportionately affect lowincome voters people of color students the elderly\n",
    "* off donald trumps comments the textbook definition of a racist a person filled with hatred with an assault weapon we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las tres palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorHillary = TextGenerator(hillary_content, 3)\n",
    "textGeneratorHillary.process_text()\n",
    "textGeneratorHillary.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* four or five weeks ill tell you but they got access so they signed a pledge they will support i dont want their support it wont mean one vote than\n",
    "* world take advantage of us both militarily and we dont win anymore we dont win anymore we dont win with trade we dont win we dont win at anything i\n",
    "* extent is the power of weaponry its the power its the tremendous power you know years ago i mean it took them like years to build it so you know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recopilación de noticias\n",
    "\n",
    "Analizamos un gran dataset de noticias de diferentes medios de comunicación estadounidenses, en inglés.\n",
    "\n",
    "Extraídos de https://catalog.ldc.upenn.edu/LDC97S44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_file = open(\"news.txt\")\n",
    "news_content = \" \".join(news_file.readlines())\n",
    "textGeneratorNews = TextGenerator(news_content, 3)\n",
    "textGeneratorNews.process_text()\n",
    "\n",
    "number_of_words = textGeneratorNews.number_of_words()\n",
    "number_of_different_words = textGeneratorNews.number_of_different_words()\n",
    "print(\"Number of words: \" + str(number_of_words) + \". Number of different words: \" + str(number_of_different_words))\n",
    "\n",
    "textGeneratorNews.plot_most_important_words(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta la palabra anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorNews = TextGenerator(news_content, 1)\n",
    "textGeneratorNews.process_text()\n",
    "textGeneratorNews.generate_text(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* plunk down threats as a search platforms microsoft will need to make sure some jewish\n",
    "* syndicate was the chinese officials said on the principle of plotting a third avenue stores\n",
    "* nonsectarian cards and international boxing final quot ordeal ugly and explore the whole truth about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las dos palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorNews = TextGenerator(news_content, 2)\n",
    "textGeneratorNews.process_text()\n",
    "textGeneratorNews.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* by stepping up quota compliance to thwart a takeover investors still got to celebrate a historic development in the computing\n",
    "* sunday liberals celebrate as ukraine waits poll result reuters reuters us house has approved a stent inserted wednesday into an\n",
    "* blade a spanish supercomputer built by china and india the shift would wreck mideast peace efforts the bush administration dramatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando texto teniendo en cuenta las tres palabras anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGeneratorNews = TextGenerator(news_content, 3)\n",
    "textGeneratorNews.process_text()\n",
    "textGeneratorNews.generate_text(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas frases generadas:\n",
    "* enough by kristen philipkoski got antinuke pills probably not potassium iodide pills could ward off cancer in the event of a nuclear accident but many states have refused to take\n",
    "* are more exposed than ever to what could be a final step on the long road to bringing the global pact into force dogs sniff out bladder cancer dogs can\n",
    "* league arsenal dropped five points behind chelsea in the summer the dutch side has scored goals in the final minutes to beat hamburg sv jol wants fulltime job at tottenham"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
